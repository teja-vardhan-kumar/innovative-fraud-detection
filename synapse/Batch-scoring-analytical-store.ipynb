{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch scoring of transactions in the analytical store\n",
        "\n",
        "In this notebook, you will retrieve transaction data from the Cosmos DB analytical store and use your trained machine learning model to predict whether each transaction is suspicious. You will then use the scored data to perform a data aggregation to determine the percentage of traffic for each `ipCountryCode` that is suspicious, and save it to the `suspicious_transactions` Cosmos DB container to be used for reporting later on."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load ML model\n",
        "\n",
        "In the last exercise, you saved your trained model to your Azure ML workspace. To perform batch scoring, the first thing you need to do is load the model from your Azure ML workspace. In the cell below, you define some helper functions for retrieving your Azure ML workspace and loading the model stored there.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Import the required libraries\n",
        "import numpy\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import azureml\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "def getOrCreateWorkspace(subscription_id, resource_group, workspace_name, workspace_region):\n",
        "    # By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace instead of an error\n",
        "    ws = Workspace.create(\n",
        "        name = workspace_name,\n",
        "        subscription_id = subscription_id,\n",
        "        resource_group = resource_group, \n",
        "        location = workspace_region,\n",
        "        exist_ok = True)\n",
        "    return ws\n",
        "\n",
        "def loadModelFromAML(ws, model_name=\"batch-score\"):\n",
        "  # download the model folder from AML to the current working directory\n",
        "  model_file_path = Model.get_model_path(model_name, _workspace=ws)\n",
        "  print('Loading model from:', model_file_path)\n",
        "  model = joblib.load(model_file_path)\n",
        "  return model"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the cell below to load your model. You will need to respond to the prompt in the output, navigating to <https://microsoft.com/devicelogin> and then entering the code specified below to authenticate.\n",
        "\n",
        "**Enter the same values** you copied from the **Prepare batch scoring model** Azure ML notebook."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Provide the Subscription ID of your existing Azure subscription\n",
        "subscription_id = \"\" #\"YOUR_SUBSCRIPTION_ID\"\n",
        "\n",
        "#Provide values for the Resource Group and Workspace that will be created\n",
        "resource_group = \"\" #\"YOUR_RESOURCE_GROUP\"\n",
        "workspace_name = \"\" #\"YOUR_AML_WORKSPACE_NAME\"\n",
        "workspace_region = \"\" # eastus, westcentralus, southeastasia, australiaeast, westeurope\n",
        "\n",
        "#Get an AML Workspace\n",
        "ws =  getOrCreateWorkspace(subscription_id, resource_group, \n",
        "                   workspace_name, workspace_region)\n",
        "\n",
        "model = loadModelFromAML(ws)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save ML model to file system\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "model_name=\"batch-score\"\n",
        "\n",
        "# Save the model for future use\n",
        "filename = model_name + '.pkl'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# load the model \n",
        "anomaly_model = pickle.load(open(filename, 'rb'))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch score transactions\n",
        "\n",
        "With the model now loaded, the next step is to create a DataFrame containing the transactions loaded from the Azure Cosmos DB analytical store, and score each of those records using the model. As you did in the previous exercise, you will need transform that data in the `transactions` table for use by your model. Encode the transformations into custom transformers for use in a pipeline as follows:\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class NumericCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"NumericCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"NumericCleaner.transform called\")\n",
        "        X[\"localHour\"] = X[\"localHour\"].fillna(-99)\n",
        "        X[\"accountAge\"] = X[\"accountAge\"].fillna(-1)\n",
        "        X[\"numPaymentRejects1dPerUser\"] = X[\"numPaymentRejects1dPerUser\"].fillna(-1)\n",
        "        X.loc[X.loc[:,\"localHour\"] == -1, \"localHour\"] = -99\n",
        "        return X\n",
        "\n",
        "class CategoricalCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"CategoricalCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"CategoricalCleaner.transform called\")\n",
        "        X = X.fillna(value={\"cardType\":\"U\",\"cvvVerifyResult\": \"N\"})\n",
        "        X['isUserRegistered'] = X.apply(lambda row: 1 if row[\"isUserRegistered\"] == \"TRUE\" else 0, axis=1)\n",
        "        return X"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "numeric_features=[\"transactionAmountUSD\", \"localHour\", \n",
        "                  \"transactionIPaddress\", \"digitalItemCount\", \"physicalItemCount\", \"accountAge\",\n",
        "                  \"paymentInstrumentAgeInAccount\", \"numPaymentRejects1dPerUser\"\n",
        "                 ]\n",
        "\n",
        "categorical_features=[\"transactionCurrencyCode\", \"browserLanguage\", \"paymentInstrumentType\", \"cardType\", \"cvvVerifyResult\",\n",
        "                      \"isUserRegistered\"\n",
        "                     ]                           \n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('cleaner', NumericCleaner())\n",
        "])\n",
        "                               \n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('cleaner', CategoricalCleaner()),\n",
        "    ('encoder', OrdinalEncoder())])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, load the batch `transaction` data into a Spark DataFrame, covert that to a Pandas DataFrame, and then pass that data through the transformation pipeline.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Load transactions from the Cosmos DB analytical store into a Spark DataFrame\n",
        "#transactions = spark.sql(\"SELECT * FROM transactions\")\n",
        "transactions = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"WoodgroveCosmosDb\")\\\n",
        "    .option(\"spark.cosmos.container\", \"transactions\")\\\n",
        "    .load()\n",
        "\n",
        "# Remove unwanted columns from the columns collection\n",
        "cols = list(set(transactions.columns) - {'_attachments','_etag','_rid','_self','_ts','collectionType','id','ttl'})\n",
        "\n",
        "transactions = transactions.select(cols)\n",
        "\n",
        "# Get a Pandas DataFrame from the Spark DataFrame\n",
        "pandas_df = transactions.toPandas()\n",
        "\n",
        "# Transform the batch data\n",
        "preprocessed_transactions = preprocessor.fit_transform(pandas_df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Score the batch data\n",
        "\n",
        "With the batch data transformed, you are now ready to use your ML model to predict whether each transaction in the data set is suspicious. Execute the following cell to retrieve the predictions from your model.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "transactions_preds = model.predict(preprocessed_transactions)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the cell below to view the predictions. Notice that the output is in the form of an `array`. Each item in the array is associated with a record in the `transactions` batch data, based on the order of the records.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "transactions_preds"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To add the prediction results to your transaction data you can use the `tolist()` method on the array. This will assign them in order to each row columns in your Pandas DataFrame. In this case, you will add the prediction as a new column named `isSuspicious`.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pandas_df[\"isSuspicious\"] = transactions_preds.tolist()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now take a quick look to see the count of suspicious versus not suspicious records in the data set.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pandas_df['isSuspicious'].value_counts()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To enable writing the scored transaction data out to a Cosmos DB container, convert the Pandas DataFrame back to a Spark DataFrame.\n",
        "\n",
        "> You may ignore any Arrow optimization warnings if you see them.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "scored_transactions = spark.createDataFrame(pandas_df)\n",
        "\n",
        "scored_transactions.createOrReplaceTempView(\"scored_transactions_view\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "transactionCurrencyCode"
            ],
            "values": [
              "paymentInstrumentAgeInAccount"
            ],
            "yLabel": "paymentInstrumentAgeInAccount",
            "xLabel": "transactionCurrencyCode",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"paymentInstrumentAgeInAccount\":{\"AUD\":0.056944444,\"BGN\":0,\"CAD\":0.297222222,\"CNY\":0,\"DKK\":0.000694444,\"EUR\":0,\"GBP\":63.879861109,\"HKD\":0.0027777770000000004,\"HUF\":0,\"PEN\":0,\"USD\":4634.831944539999}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": true
        }
      },
      "source": [
        "%%sql\n",
        "\n",
        "SELECT * from scored_transactions_view  LIMIT 100"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write data to Azure Cosmos DB using HTAP"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aggregate data by `ipCountryCode`\n",
        "\n",
        "The final action you will perform in this notebook is to do a simple aggregation of the data based on the `isSuspicious` value and the `ipCountryCode`.\n",
        "\n",
        "We set the `id` value to `ipCountryCode` so we can perform upserts on the aggregate data. We also set the `collectionType` field to `SuspiciousAgg` to differentiate between suspicious transaction data and aggregate data in the `suspicious_transactions` container.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "suspicious_agg = spark.sql(\"SELECT ipCountryCode AS id, ipCountryCode, COUNT(CASE WHEN isSuspicious = 1 THEN 0 END) SuspiciousTransactionCount, COUNT(*) AS TotalTransactionCount, COUNT(CASE WHEN isSuspicious = 1 THEN 0 END)/COUNT(*) AS PercentSuspicious, 'SuspiciousAgg' AS collectionType FROM scored_transactions_view GROUP BY ipCountryCode ORDER BY ipCountryCode\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "suspicious_agg.orderBy(desc('PercentSuspicious')).show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, write the DataFrame to the Azure Cosmos DB `suspicious_transactions` OLTP container."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "suspicious_agg.write\\\n",
        "            .format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"WoodgroveCosmosDb\")\\\n",
        "            .option(\"spark.cosmos.container\", \"suspicious_transactions\")\\\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "            .mode('append')\\\n",
        "            .save()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "suspicious_agg.show()"
      ],
      "attachments": {}
    }
  ]
}