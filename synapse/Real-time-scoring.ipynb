{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Near real-time scoring of streaming transactions using the Cosmos DB Change Feed and Spark Structured Streaming\n",
        "\n",
        "In this notebook, you will load the batch scoring model from your Azure Machine Learning workspace and use the Cosmos DB Linked Service to connect to the [Azure Cosmos DB Change Feed](https://docs.microsoft.com/en-us/azure/cosmos-db/change-feed). Using the Cosmos DB Change Feed and [Spark Structured Streaming](https://docs.azuredatabricks.net/spark/latest/structured-streaming/index.html), you will read the data being ingested into the Cosmos DB `transactions` container and use your trained machine learning model to predict whether each transaction is suspicious, in near-real-time, as it streams in. You will then write transaction data scored as `isSuspicious` and save that to a `suspicious_transactions` Cosmos DB container.\n",
        "\n",
        "You will also select all of the transactions where `isSuspicious` is true, and write those out to your transactions collection in Azure Comsos DB.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the TransactionGenerator\n",
        "\n",
        "With the streaming query now created, it is time to restart the transaction generator. In Exercise 1 of the hands-on lab step-by-step guide, you used the `TransactionGenerator` console app on your LabVM to compare data ingestion between Event Hubs and Cosmos DB. You will now make a configuration change in the console app's `appsettings.js` file to send transactions only to Cosmos DB. These transactions will be used to enable reading from the Cosmos DB Change Feed as new transactions are inserted into the Comsos DB collection.\n",
        "\n",
        "1. Return to your LabVM and the TransactionGenerator console app.\n",
        "2. Open the `appSettings.js` file and locate the `ONLY_WRITE_TO_COSMOS_DB` setting.\n",
        "3. Change the value of `ONLY_WRITE_TO_COSMOS_DB` to `true`.\n",
        "4. Save `appSettings.js`.\n",
        "5. Start the console app by selecting the green **Run** button in the Visual Studio toolbar.\n",
        "6. Once the console app is running, return to this notebook and continue on with the next cell.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load ML model\n",
        "\n",
        "In the last exercise, you saved your trained model to your Azure ML workspace. To perform batch scoring, the first thing you need to do is load the model from your Azure ML workspace. In the cell below, you define some helper functions for retrieving your Azure ML workspace and loading the model stored there.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Import the required libraries\r\n",
        "import numpy\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import pickle\r\n",
        "import azureml\r\n",
        "from azureml.core import Workspace\r\n",
        "from azureml.core.model import Model\r\n",
        "from sklearn.externals import joblib\r\n",
        "\r\n",
        "def getOrCreateWorkspace(subscription_id, resource_group, workspace_name, workspace_region):\r\n",
        "    # By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace instead of an error\r\n",
        "    ws = Workspace.create(\r\n",
        "        name = workspace_name,\r\n",
        "        subscription_id = subscription_id,\r\n",
        "        resource_group = resource_group, \r\n",
        "        location = workspace_region,\r\n",
        "        exist_ok = True)\r\n",
        "    return ws\r\n",
        "\r\n",
        "def loadModelFromAML(ws, model_name=\"batch-score\"):\r\n",
        "  # download the model folder from AML to the current working directory\r\n",
        "  model_file_path = Model.get_model_path(model_name, _workspace=ws)\r\n",
        "  print('Loading model from:', model_file_path)\r\n",
        "  model = joblib.load(model_file_path)\r\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the cell below to load your model. You will need to respond to the prompt in the output, navigating to <https://microsoft.com/devicelogin> and then entering the code specified below to authenticate.\n",
        "\n",
        "**Enter the same values** you copied from the **Prepare batch scoring model** Azure ML notebook.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Provide the Subscription ID of your existing Azure subscription\n",
        "subscription_id = \"\" #\"YOUR_SUBSCRIPTION_ID\"\n",
        "\n",
        "#Provide values for the Resource Group and Workspace that will be created\n",
        "resource_group = \"\" #\"YOUR_RESOURCE_GROUP\"\n",
        "workspace_name = \"\" #\"YOUR_AML_WORKSPACE_NAME\"\n",
        "workspace_region = \"\" # eastus, westcentralus, southeastasia, australiaeast, westeurope\n",
        "\n",
        "#Get an AML Workspace\n",
        "ws =  getOrCreateWorkspace(subscription_id, resource_group, \n",
        "                   workspace_name, workspace_region)\n",
        "\n",
        "model = loadModelFromAML(ws)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save ML model to file system\n",
        "\n",
        "In the next task, you will be creating a Databricks Job to run batch scoring on a schedule. To facilitate the use of your model by this scheduled job, it is easiest to save a copy of the model into a shared folder in DBFS within your Databricks workspace. Databricks spins up a new cluster every time a schduled job runs, so this will prevent your job from needing to authenticate against your AML workspace each time that happens.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_name=\"batch-score\"\n",
        "\n",
        "# Save the model for future use\n",
        "filename = model_name + '.pkl'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load the model \n",
        "anomaly_model = pickle.load(open(filename, 'rb'))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch score transactions\n",
        "\n",
        "With the model now loaded, the next step is to create a DataFrame containing the transactions loaded from the Azure Cosmos DB analytical store, and score each of those records using the model. As you did in the previous exercise, you will need transform that data in the `transactions` table for use by your model. Encode the transformations into custom transformers for use in a pipeline as follows:\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class NumericCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"NumericCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"NumericCleaner.transform called\")\n",
        "        X[\"localHour\"] = X[\"localHour\"].fillna(-99)\n",
        "        X[\"accountAge\"] = X[\"accountAge\"].fillna(-1)\n",
        "        X[\"numPaymentRejects1dPerUser\"] = X[\"numPaymentRejects1dPerUser\"].fillna(-1)\n",
        "        X.loc[X.loc[:,\"localHour\"] == -1, \"localHour\"] = -99\n",
        "        return X\n",
        "\n",
        "class CategoricalCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"CategoricalCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"CategoricalCleaner.transform called\")\n",
        "        X = X.fillna(value={\"cardType\":\"U\",\"cvvVerifyResult\": \"N\"})\n",
        "        X['isUserRegistered'] = X.apply(lambda row: 1 if row[\"isUserRegistered\"] == \"TRUE\" else 0, axis=1)\n",
        "        return X"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.preprocessing import OrdinalEncoder\r\n",
        "\r\n",
        "numeric_features=[\"transactionAmountUSD\", \"localHour\", \r\n",
        "                  \"transactionIPaddress\", \"digitalItemCount\", \"physicalItemCount\", \"accountAge\",\r\n",
        "                  \"paymentInstrumentAgeInAccount\", \"numPaymentRejects1dPerUser\"\r\n",
        "                 ]\r\n",
        "\r\n",
        "categorical_features=[\"transactionCurrencyCode\", \"browserLanguage\", \"paymentInstrumentType\", \"cardType\", \"cvvVerifyResult\",\r\n",
        "                      \"isUserRegistered\"\r\n",
        "                     ]                           \r\n",
        "\r\n",
        "numeric_transformer = Pipeline(steps=[\r\n",
        "    ('cleaner', NumericCleaner())\r\n",
        "])\r\n",
        "                               \r\n",
        "categorical_transformer = Pipeline(steps=[\r\n",
        "    ('cleaner', CategoricalCleaner()),\r\n",
        "    ('encoder', OrdinalEncoder())])\r\n",
        "\r\n",
        "preprocessor = ColumnTransformer(\r\n",
        "    transformers=[\r\n",
        "        ('num', numeric_transformer, numeric_features),\r\n",
        "        ('cat', categorical_transformer, categorical_features)\r\n",
        "    ])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, load the batch `transaction` data into a Spark DataFrame, covert that to a Pandas DataFrame, and then pass that data through the transformation pipeline.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stream transactions from Cosmos DB Change Feed into Azure Databricks\r\n",
        "\r\n",
        "Change feed support in Azure Cosmos DB works by listening to an Azure Cosmos DB container for any changes. It then outputs the sorted list of documents that were changed in the order in which they were modified. The changes are persisted, can be processed asynchronously and incrementally, and the output can be distributed across one or more consumers for parallel processing.\r\n",
        "\r\n",
        "In the cell below, a configuration object is created containing the required information for connecting to the Cosmos DB Change Feed. \r\n",
        "\r\n",
        "That configuration is then used in conjunction with the `readStream` command for Structured Streaming to stream records from the Cosmos DB Change Feed into a DataFrame.\r\n",
        "\r\n",
        "> **NOTE**: The `format` property used below is different when reading streaming data from Cosmos DB than we reading batch or static data. In the case of streaming data, you use `cosmos.oltp`.\r\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create scoring and saving functions\n",
        "\n",
        "To leverage the model for scoring microbatches, you need to define a function with the scoring logic that will write the scored results to your desired destination. \n",
        "\n",
        "In this case, you will write the scored results out to a different Azure Cosmos DB container: **`suspicious_transactions`**. This is a different container than the `transactions` container from which we are reading from the change feed. We store only those transactions that have been predicted as suspicious to the **`suspicious_transactions`** container, which meets our criteria for globally serving the data to customers.\n",
        "\n",
        "Run the following cells that will create the helper functions for scoring and saving.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "def foreach_batch_scorer(df, epoch_id):\r\n",
        "    # Transform and write batchDF\r\n",
        "    scored_df = score_batch(df)\r\n",
        "    write_scored_results(scored_df)\r\n",
        "    pass\r\n",
        "\r\n",
        "def score_batch(df):\r\n",
        "    # Remove unneeded columns\r\n",
        "    cols = list(set(df.columns) - {'_attachments','_etag','_rid','_self','_ts','collectionType','id','ttl'})\r\n",
        "    changes_clean = df.select(cols)\r\n",
        "    # Get a Pandas DataFrame from the Spark DataFrame\r\n",
        "    pandas_df = changes_clean.toPandas()\r\n",
        "    # Transform the batch data\r\n",
        "    preprocessed_transactions = preprocessor.fit_transform(pandas_df)\r\n",
        "    transactions_preds = model.predict(preprocessed_transactions)\r\n",
        "    #pandas_df[\"isSuspicious\"] = transactions_preds.tolist()\r\n",
        "    pandas_df[\"isSuspicious\"] = transactions_preds\r\n",
        "    pandas_df[\"collectionType\"] = 'SuspiciousTransactions'\r\n",
        "    scored_transactions = spark.createDataFrame(pandas_df)\r\n",
        "    scored_transactions = (scored_transactions\r\n",
        "        .filter( col(\"isSuspicious\") == True))\r\n",
        "    return scored_transactions\r\n",
        "\r\n",
        "def write_scored_results(scored_df):\r\n",
        "    (scored_df\r\n",
        "        .write\r\n",
        "        .format(\"cosmos.oltp\")\r\n",
        "        .option(\"spark.synapse.linkedService\", \"WoodgroveCosmosDb\")\r\n",
        "        .option(\"spark.cosmos.container\", \"suspicious_transactions\")\r\n",
        "        .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\r\n",
        "        .mode('append')\r\n",
        "        .save()\r\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read streaming data from the change feed\n",
        "\n",
        "Let's quickly examine the command's components:\n",
        "\n",
        "  - `format(\"cosmos.oltp\")`: This specifies that we want to read from the transactional store.\n",
        "  - `option(\"spark.synapse.linkedService\")`: The name of the Cosmos DB Linked Service.\n",
        "  - `option(\"spark.cosmos.container\")`: The name of the Cosmos DB container from which to read the streaming data.\n",
        "  - `option(\"spark.cosmos.changeFeed.startFromTheBeginning\")`: This specifies whether to read from the beginning of the change feed, rather than from the point at which you execute the cell.\n",
        "  - `option(\"checkpointLocation\")`: By specifying a checkpoint location, you ensure that any interruption to the writeStream operation can be continued where it left off.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dfStream = spark.readStream\\\n",
        "    .format(\"cosmos.oltp\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"WoodgroveCosmosDb\")\\\n",
        "    .option(\"spark.cosmos.container\", \"transactions\")\\\n",
        "    .option(\"spark.cosmos.changeFeed.readEnabled\", \"true\")\\\n",
        "    .option(\"spark.cosmos.changeFeed.startFromTheBeginning\", \"true\")\\\n",
        "    .option(\"spark.cosmos.changeFeed.checkpointLocation\", \"/localReadCheckpointFolder\")\\\n",
        "    .option(\"spark.cosmos.changeFeed.queryName\", \"streamQuery\")\\\n",
        "    .load()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrames have an `isStreaming` property, which you can use to verify the `dfStream` DataFrame created above is a streaming DataFrame.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dfStream.isStreaming"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to start the streaming query. Notice the use of `foreachBatch` as sink (the destination for the data)."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "streaming_query = dfStream.writeStream.foreachBatch(foreach_batch_scorer).start() "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop the streaming query\n",
        "\n",
        "Allow the TransactionGenerator to run until completion, and then execute the cell below to stop the streaming query."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "streaming_query.stop()"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}