{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51832410-a165-43b7-92dc-7abbe11f0167",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prepare the real-time scoring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a7492ab-365b-4281-877d-b648e1b6f2af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade azureml-train-automl-runtime==1.36.0\n",
    "#!pip install --upgrade azureml-automl-runtime==1.36.0\n",
    "#!pip install --upgrade scikit-learn\n",
    "#!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f9d677-e760-48fe-88cf-ba34192f1ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "pip install azureml-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57616c45-7753-400e-8e75-fcf5bb7f032a",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511718401
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# sklearn.externals.joblib was deprecated in 0.21\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.21.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ws = Workspace.get(name='fd-machine-learning-workspace',\n",
    "                    subscription_id= '831359a3-06f6-4ed7-a3d0-535b8b673781',\n",
    "                    resource_group='fd-resource-group')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa9f857-9f59-4554-98fa-81ef5621a7ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account_df = pd.read_csv('../data/Account_Info.csv')\n",
    "fraud_df = pd.read_csv('../data/Fraud_Transactions.csv')\n",
    "untagged_df = pd.read_csv( '../data/Untagged_Transactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2852d3d5-b605-4a12-86c3-58e7e7a3470f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "View the fraud dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35c7c49-a2fa-4181-8fc1-eb3d07ddc739",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fraud_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71f721ba-82f9-49aa-88e8-0e489d5bcedd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "View the account info dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df900c8-fc43-44e3-aa80-2d2437b89bcc",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511718776
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "account_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b9ad61-65a1-4cad-8953-3ee2cd3c9c27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "View the untagged transactions dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2719e13-2233-43bf-97b1-fa461e7233fd",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511718973
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Reorder the column of dataframe by ascending order in pandas \n",
    "cols=untagged_df.columns.tolist()\n",
    "cols.sort()\n",
    "untagged_df=untagged_df[cols]\n",
    "\n",
    "untagged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c51455-0452-44af-bdb5-36dcb34eddd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prepare data\n",
    "\n",
    "The raw data has some issues we need to cleanup before we can use it to train a model, which we perform in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd61b8d9-b46b-4cdb-881a-c40b620090d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare accounts\n",
    "\n",
    "Begin by cleaning the data in accounts data set.\n",
    "Remove columns that have very few or no values: `accountOwnerName`, `accountAddress`, `accountCity` and `accountOpenDate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378de8f3-8173-4433-8500-13faa776a875",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511719101
    }
   },
   "outputs": [],
   "source": [
    "account_df_clean = account_df[[\"accountID\", \"transactionDate\", \"transactionTime\", \n",
    "                               \"accountPostalCode\", \"accountState\", \"accountCountry\", \n",
    "                               \"accountAge\", \"isUserRegistered\", \"paymentInstrumentAgeInAccount\", \n",
    "                               \"numPaymentRejects1dPerUser\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f29834-69ad-4dee-aa42-2d3754e235f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a copy of the dataframe so our data manipulation does not affect the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc15515d-789e-43e7-bdad-025bee5980f2",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511719240
    }
   },
   "outputs": [],
   "source": [
    "account_df_clean = account_df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f84680-74e0-4c86-a7f8-d81299d11da1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's ensure that values that are not numeric (e.g., they have incorrect string values or garbage data) are converted to NaN and then we can fill those NaN values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe9b9d3-efa9-4d41-8430-b2d7ef589469",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511719390
    }
   },
   "outputs": [],
   "source": [
    "account_df_clean['paymentInstrumentAgeInAccount'] = pd.to_numeric(account_df_clean['paymentInstrumentAgeInAccount'], errors='coerce')\n",
    "account_df_clean['paymentInstrumentAgeInAccount'] = account_df_clean[['paymentInstrumentAgeInAccount']].fillna(0)['paymentInstrumentAgeInAccount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3fbdf5a-2f63-467d-9c68-13c5a7d6a800",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, let's convert the `numPaymentRejects1dPerUser` so that the column has a datatype of `float` instead of `object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9cf764-c449-491d-b390-f72fa46c6baf",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511719499
    }
   },
   "outputs": [],
   "source": [
    "account_df_clean[\"numPaymentRejects1dPerUser\"] = account_df_clean[[\"numPaymentRejects1dPerUser\"]].astype(float)[\"numPaymentRejects1dPerUser\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c2d565-29e4-460e-afe6-c5990dfcf377",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511719630
    }
   },
   "outputs": [],
   "source": [
    "account_df_clean[\"numPaymentRejects1dPerUser\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09b3f2a-d12f-4a4e-a297-6f491853bccf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`account_df_clean` is now ready for use in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05bf95f1-638d-46c6-9094-c5c97ad335b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare untagged transactions\n",
    "\n",
    "Next, cleanup the untagged transactions data set. There are 16 columns in the untagged_transactions whose values are all null, let's drop these columns to simplify our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770835cc-bf38-4e85-9a1b-f59ac20bd054",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511719849
    }
   },
   "outputs": [],
   "source": [
    "untagged_df_clean = untagged_df.dropna(axis=1, how=\"all\").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c98971c3-687a-464b-80a9-a5d806448bec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can examine the count of non-null values, and view the inferred data type for each column by running the following cell. Looking at the output of the cell, we have some work to do. For a start, we have columns with fewer than 200,000 non-null values. This means there are some null values in that column that we need to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cde32c3-9b77-4a58-9bb4-7fed8854f8a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's cleanup the `localHour` field. \n",
    "\n",
    "Replace null values in `localHour` with `-99`. Also replace values of `-1` with `-99`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f4b210-3e75-4349-a6be-5ab4df48742b",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511720100
    }
   },
   "outputs": [],
   "source": [
    "untagged_df_clean[\"localHour\"] = untagged_df_clean[\"localHour\"].fillna(-99)\n",
    "untagged_df_clean.loc[untagged_df_clean.loc[:,\"localHour\"] == -1, \"localHour\"] = -99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "082de201-36e0-4d02-867c-3326a5dd072b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Confirm the values now look good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01b5786-d5e8-40ff-b73d-5843d1fd4316",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511720219
    }
   },
   "outputs": [],
   "source": [
    "untagged_df_clean[\"localHour\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90da6895-6ba5-48af-ba05-2f1080c005d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Clean up the remaining null fields:\n",
    "- Fix missing values for location fields by setting them to `NA` for unknown. \n",
    "- Set `isProxyIP` to False\n",
    "- Set `cardType` to `U` for unknown (which is a new level)\n",
    "- Set `cvvVerifyResult` to `N` which means for those where the transaction failed because the wrong CVV2 number was entered ro no CVV2 numebr was entered, treat those as if there was no CVV2 match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c62afb-03b0-4ee3-81c2-7a859633f842",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511720330
    }
   },
   "outputs": [],
   "source": [
    "untagged_df_clean = untagged_df_clean.fillna(value={\"ipState\": \"NA\", \"ipPostcode\": \"NA\", \"ipCountryCode\": \"NA\", \n",
    "                               \"isProxyIP\":False, \"cardType\": \"U\", \n",
    "                               \"paymentBillingPostalCode\" : \"NA\", \"paymentBillingState\":\"NA\",\n",
    "                               \"paymentBillingCountryCode\" : \"NA\", \"cvvVerifyResult\": \"N\"\n",
    "                              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a168786-3e1c-4351-b24c-92872fca739b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Confirm all null values have been addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd57443d-7b1c-4b3b-80c2-a8d50ae3a74c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `transactionScenario` column provides no insights because all rows have the same `A` value. Let's drop that column. Same idea for the `transactionType` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1dea21-46a7-47b3-a71d-71aa10facba9",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511720628
    }
   },
   "outputs": [],
   "source": [
    "del untagged_df_clean[\"transactionScenario\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10edced1-41d7-43d8-a76e-0918fb19e84f",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511720733
    }
   },
   "outputs": [],
   "source": [
    "del untagged_df_clean[\"transactionType\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86ae1c39-5763-4b50-af54-f40e5c911e04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`untagged_df_clean` is now ready for use in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b83d6b2-557a-4916-8476-6f7507b8e011",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare fraud transactions\n",
    "\n",
    "Now move on to preparing the fraud transactions data set.\n",
    "\n",
    "The `transactionDeviceId` has no meaningful values, so we will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8795031b-1545-4b46-8e7d-7c679a7e0a99",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511720840
    }
   },
   "outputs": [],
   "source": [
    "fraud_df_clean = fraud_df.copy()\n",
    "del fraud_df_clean['transactionDeviceId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b88748a4-eb53-4edf-99ba-59a399ccf5cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The fraud data set has a `localHour` field that we need to fill missing values, just as we did for the account data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f6db66-4dd8-4b90-97e6-7ca8f7ae3b8d",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511721027
    }
   },
   "outputs": [],
   "source": [
    "fraud_df_clean[\"localHour\"] = fraud_df_clean[\"localHour\"].fillna(-99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c012853-1e87-47c8-8a8b-f6467e50bb2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Examine your work, you should have 8640 non-null values in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6f048ca-a23c-4631-a5af-c3da77fa642c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`fraud_df_clean` is now ready for use in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21e38b7c-389f-4279-9286-df3f4823c4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create labels\n",
    "\n",
    "The goal is to create a dataframe with all transactions, where each transaction is tagged via the `isFraud` column with a value of `0` - no fraud or `1` - fraudulent. \n",
    "\n",
    "Any transactions that appear in untagged_transactions dataframe that also appear in the fraud dataframe will be marked as fraudulent. \n",
    "\n",
    "The remaining transactions will be marked as not fraudulent. \n",
    "\n",
    "Run the following cells to create the labels series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb0fb75-bf4a-4972-adbb-d3928bf47979",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511721246
    }
   },
   "outputs": [],
   "source": [
    "all_labels = untagged_df_clean[\"transactionID\"].isin(fraud_df_clean[\"transactionID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574e00fb-897f-463b-90ec-839ad9453dfe",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511721358
    }
   },
   "outputs": [],
   "source": [
    "all_transactions = untagged_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4b4b40-2f98-429d-a242-6cbaf6c803d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Then we can save our estimators module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ede44a-bbfe-4a9d-b2b4-ad55dc6325e1",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511721585
    }
   },
   "outputs": [],
   "source": [
    "# write out to models/customestimators.py\n",
    "scoring_service = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class NumericCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"NumericCleaner.fit called\")\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(\"NumericCleaner.transform called\")\n",
    "        X[\"localHour\"] = X[\"localHour\"].fillna(-99)\n",
    "        X.loc[X.loc[:,\"localHour\"] == -1, \"localHour\"] = -99\n",
    "        return X\n",
    "\n",
    "class CategoricalCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"CategoricalCleaner.fit called\")\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(\"CategoricalCleaner.transform called\")\n",
    "        X = X.fillna(value={\"cardType\":\"U\",\"cvvVerifyResult\": \"N\"})\n",
    "        return X\n",
    "\"\"\" \n",
    "\n",
    "with open(\"./customestimators.py\", \"w\") as file:\n",
    "    file.write(scoring_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54b4628-2276-4cdd-94fa-d74a3faf0815",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, load the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9147fe2-ab09-4c92-8717-78eb33025f67",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511721824
    }
   },
   "outputs": [],
   "source": [
    "from customestimators import NumericCleaner, CategoricalCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca8c003a-9c0d-4717-955b-bfd12742febb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now build the pipeline that will prepare the data. \n",
    "\n",
    "The gist of the following cell is to split the data preparation into two paths, splitting the data sets vertically, and then combine the result. The `ColumnTransformer` will effectively concatenate the data frame that results from the numeric transformations with the data frame resulting from the categorical transformations. \n",
    "\n",
    "- Numeric Transformer Pipeline: We use the custom transformers created previously to cleanup the numeric columns. Since the model you will train in this notebook is a Support Vector Machine classifier, we need to standardize the scale of numeric values which is what the `StandardScaler` provides.\n",
    "- Categorical Transformer Pipeline: We use the custome transformer created previously cleanup the categorical columns. Then we one-hot encode each value of each categorical column, resulting in a wider data frame with one column for each possible value (and 1 appearing in rows that had that value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdaaa4d-38bd-4948-ba95-799d87a71af7",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511721941
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "numeric_features=[\"transactionAmountUSD\", \"transactionDate\", \"transactionTime\", \"localHour\", \n",
    "                  \"transactionIPaddress\", \"digitalItemCount\", \"physicalItemCount\"]\n",
    "\n",
    "categorical_features=[\"transactionCurrencyCode\", \"browserLanguage\", \"paymentInstrumentType\", \"cardType\", \"cvvVerifyResult\"]                           \n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('cleaner', NumericCleaner()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "                               \n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('cleaner', CategoricalCleaner()),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5181967-6717-4fd8-b117-b294b951a81b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's confirm we run all our historical data thru this transformation pipeline and observe the resulting shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb76c4c-564b-400f-9c11-6b668b5e572c",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511722317
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_result = preprocessor.fit_transform(all_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ec2e3d-620c-420f-87fa-28a9e8fe2730",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511722445
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81db0264-ad33-4cfb-b25e-03e2329f62b6",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511722575
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessed_result.todense()).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db308e79-5d61-4b61-892e-88c412f8ed6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create pipeline and train a simple model\n",
    "\n",
    "Now you will build upon the transformation pipeline you created previously to train a model to classify rows as fraudulent or not fraudulent.\n",
    "\n",
    "Run the following cells to make sure you've imported the dependencies for the pipeline (you probably already have, but having them clearly loaded here will help you when porting your code to a web service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33b2085-5f2f-4e44-ab9f-4715f0025e63",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511722677
    }
   },
   "outputs": [],
   "source": [
    "from customestimators import NumericCleaner, CategoricalCleaner\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e8ede18-fc4d-4882-b98d-76ac6257e230",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As might be obvious, our data has a lot of samples that are not fraudulent. If we proceed to train a model, we will effectively train the model to predict non-fraud. This situation where one class (non-fraud) appears much more often than the others (fraud) is called a class imbalance, and to mitigate its effect we can reduce the number of non-fraud samples so that we have the same number of non-fraud and fraud samples. \n",
    "\n",
    "Run the following cells to downsize and then randomly sample 1,151 non-fraud rows, and then we'll union these row with our 1,151 fraud rows.\n",
    "\n",
    "> Feel free to ignore any `SettingWithCopyWarning` warnings in the cell output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db901d7a-cd35-4931-a3d2-de6cf251aead",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511722943
    }
   },
   "outputs": [],
   "source": [
    "only_fraud_samples = all_transactions.loc[all_labels == True]\n",
    "only_fraud_samples[\"label\"] = True\n",
    "only_non_fraud_samples = all_transactions.loc[all_labels == False]\n",
    "only_non_fraud_samples[\"label\"] = False\n",
    "random_non_fraud_samples = only_non_fraud_samples.sample(n=1151, replace=False, random_state=42)\n",
    "balanced_transactions = pd.concat([random_non_fraud_samples, only_fraud_samples])\n",
    "\n",
    "balanced_transactions[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cd9e679-b9fe-43d5-be05-f8e4c1f06111",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, you need to separate out the label column from the dataframe so the labels are not used as input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d2188d-e8f5-4be4-b48e-3fbdbd8fbbe4",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723055
    }
   },
   "outputs": [],
   "source": [
    "balanced_labels = balanced_transactions[\"label\"]\n",
    "del balanced_transactions[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d171814-44ab-4947-9f36-5b43b40d4e30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now you will create subsets of the training data frame, one that will be used for training the model `X_train` and `y_train` and the another that reserved for testing its performance `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82a5b7c-7cd6-4465-be8e-351cdb6cb863",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723155
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(balanced_transactions, balanced_labels, \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "646c94f6-fdbd-4b2d-a031-f421749b5aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now train the model. In this case, you will use the `LinearSVC` class.\n",
    "\n",
    "> Feel free to ignore any `ConvergenceWarning` warnings in the cell output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3982cf5-5e6f-4b9a-8884-f656bc4c173f",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723343
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_clf = Pipeline((\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "))\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a6547e-1103-4291-94db-f135c42216f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test the model predicting against a single row from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da031a0f-eb55-46a3-8d6d-1b5b8b298658",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723457
    }
   },
   "outputs": [],
   "source": [
    "svm_clf.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24210c0-4d0c-47a7-bd9d-f2c488b2845c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, evaluate the model by examining how well it is predicting against all data in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d334348-9290-4d3e-a6aa-ef34ff2a21e0",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723564
    }
   },
   "outputs": [],
   "source": [
    "y_train_preds = svm_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ad33c3a-675a-42da-9667-a08f38eb2755",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use a confusion matrix to see how your model performed when correctly predicting non-fraud and fraud (the top left and bottom right values). Also, examine how your model made mistakes (the bottom left and top right values). In the below, the column headers are predicted non-fraud and predicted fraud, and the row headers are actually non-fraud, and actually fraud (e.g., as described by the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b718d6-82b1-49d8-aec5-f30798e42d4a",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723681
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "confusion_matrix(y_train, y_train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c9cacf3-0dab-4f3d-81f3-6a6d5680fa17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Take a look at the performance of your model using the common set of metrics for a classifier. Do you think this is good or bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d44525-d134-4714-9b2a-542284608d73",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723874
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_train, y_train_preds))\n",
    "print(\"Precision:\", precision_score(y_train, y_train_preds))\n",
    "print(\"Recall:\", recall_score(y_train, y_train_preds))\n",
    "print(\"F1:\", f1_score(y_train, y_train_preds))\n",
    "print(\"AUC:\", roc_auc_score(y_train, y_train_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e972fb95-f1c5-460d-9583-1db5c881ff25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Given that this is just a parsimonous model, this model provides a start that performs better than random (as indicated by the AUC being greater than 0.5). There is more work (such as additional feature engineering) that can be done to improve this beyond the current performance that you would want to do before deploying it in production. A parsiminous model helps us to both see if the desired classification is possible given the data and allows to quickly get to something we can deploy as a service to enable integration early on. Then we can iterate deploying improved versions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76735921-4430-403d-902e-3753fa0ee80c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, evaluate the same using the test data set, using data the trained model has not seen. How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595269d2-c0f3-464a-a3b8-dbe941def814",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511723990
    }
   },
   "outputs": [],
   "source": [
    "y_test_preds = svm_clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_test_preds))\n",
    "print(accuracy_score(y_test, y_test_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_preds))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_preds))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_preds))\n",
    "print(\"F1:\", f1_score(y_test, y_test_preds))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d39cfcf-579e-463b-9160-85d996556d12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The overall performance of the model against data it has not seen (the test data) is similar to how it performs with the training data. That's a good sign, indicating we did not overfit the model to the training data.\n",
    "\n",
    "Next, let's look the steps to prepare the model for deployment as a web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e994ff-3c2f-4aaf-83d5-d4b7523855ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save the model to disk\n",
    "\n",
    "In preparation for deploying the model, you need to save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c588ef81-11e0-4f93-9c03-6a4bbe9bc52c",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511724126
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(svm_clf, 'fraud_score.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ef1c97b-ea79-48c3-bf18-8c3559938aaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test loading the model\n",
    "\n",
    "Next simulate re-loading the model from disk, just like the web service (which you will create in a moment) will have to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af76b8cb-6320-4b3f-8b9e-bf045cb288f3",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511724254
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from customestimators import NumericCleaner, CategoricalCleaner\n",
    "\n",
    "# sklearn.externals.joblib was deprecated in 0.21\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.21.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib\n",
    "\n",
    "desired_cols = ['accountID',\n",
    " 'browserLanguage',\n",
    " 'cardType',\n",
    " 'cvvVerifyResult',\n",
    " 'digitalItemCount',\n",
    " 'ipCountryCode',\n",
    " 'ipPostcode',\n",
    " 'ipState',\n",
    " 'isProxyIP',\n",
    " 'localHour',\n",
    " 'paymentBillingCountryCode',\n",
    " 'paymentBillingPostalCode',\n",
    " 'paymentBillingState',\n",
    " 'paymentInstrumentType',\n",
    " 'physicalItemCount',\n",
    " 'transactionAmount',\n",
    " 'transactionAmountUSD',\n",
    " 'transactionCurrencyCode',\n",
    " 'transactionDate',\n",
    " 'transactionID',\n",
    " 'transactionIPaddress',\n",
    " 'transactionTime']\n",
    "\n",
    "scoring_pipeline = joblib.load('fraud_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af62925-b1be-40c6-a09f-1c8f0282f861",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511724553
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "untagged_df_fresh = pd.read_csv('./data/Untagged_Transactions.csv')[desired_cols]\n",
    "\n",
    "test_pipeline_preds = scoring_pipeline.predict(untagged_df_fresh)\n",
    "test_pipeline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd91427-81f2-4915-b4f2-b88aa389a6b5",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1613511724654
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "one_row = untagged_df_fresh.iloc[:1]\n",
    "test_pipeline_preds2 = scoring_pipeline.predict(one_row)\n",
    "test_pipeline_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b0132d-5c98-4387-b281-a1a4d6c74dec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# sklearn.externals.joblib was deprecated in 0.21\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.21.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3972d1a-d869-4b15-88ab-a2b061645df2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def saveModelToAML(ws, model, model_folder_path=\"models\", model_name=\"realtime-score\"):\n",
    "    # create the models subfolder if it does not exist in the current working directory\n",
    "    target_dir = './' + model_folder_path\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "  \n",
    "    # save the model to disk\n",
    "    joblib.dump(model, model_folder_path + '/' + model_name + '.pkl')\n",
    "  \n",
    "    # notice for the model_path, we supply the name of the model outputs folder without a trailing slash\n",
    "    # anything present in the model folder path will be uploaded to AML along with the model\n",
    "    print(\"Registering and uploading model...\")\n",
    "    registered_model = Model.register(model_path=model_folder_path, \n",
    "                                      model_name=model_name, \n",
    "                                      workspace=ws)\n",
    "    return registered_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3869fe3f-04bf-4ed8-a21e-c19d6a6ebe77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def loadModelFromAML(ws, model_name=\"realtime-score\"):\n",
    "  # download the model folder from AML to the current working directory\n",
    "  model_file_path = Model.get_model_path(model_name, _workspace=ws)\n",
    "  print('Loading model from:', model_file_path)\n",
    "  model = joblib.load(model_file_path)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2be5087-4016-4876-bb30-63d3b9d3eb1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Save the model to the AML Workspace\n",
    "registeredModel = saveModelToAML(ws, svm_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc24439b-255b-4c72-a792-487051ba091b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbct = loadModelFromAML(ws)\n",
    "y_test_preds = gbct.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168ff6cd-9969-480e-bfb0-60a019a7c31b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91e8c93a-eb0c-4680-be9f-2619cbdf89bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3524426112932497,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Prepare real-time scoring model",
   "widgets": {}
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a7a2d454ac7556fd6d46ded514fdcda5ba33f1a47e4241bd3827efe4236550a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
